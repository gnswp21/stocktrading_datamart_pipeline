{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType\n",
    "import pydeequ\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"finance_data_etl\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "df_stock_AAPL = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferschema', 'true')\\\n",
    "    .load('input/stock/AAPL_data.csv')\\\n",
    "    .drop('_c0')\n",
    "    \n",
    "\n",
    "\n",
    "df_stock_AAPL.show(3)\n",
    "\n",
    "df_stock_AAPL.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/strading\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"stock_AAPL\",\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "# budget data\n",
    "budget_schema = StructType([\n",
    "    StructField('budget_id', IntegerType(), nullable=False),\n",
    "    StructField('budget', IntegerType(), nullable=False)]\n",
    ")\n",
    "\n",
    "\n",
    "df_budget = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .schema(budget_schema) \\\n",
    "    .load('input/budget.csv') \\\n",
    "    .withColumnRenamed(\"계좌id\", \"budget_id\") \\\n",
    "    .withColumnRenamed(\"예산\", \"budget\")\n",
    "\n",
    "df_budget.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_budget.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "\n",
    "df_budget.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/strading\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"budget\",\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer data\n",
    "customer_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), nullable=False),\n",
    "    StructField('name', StringType(), nullable=False),\n",
    "    StructField('sex', StringType(), nullable=True),\n",
    "    StructField('age', IntegerType(), nullable=True),\n",
    "    StructField('budget_id', IntegerType(), nullable=True),]\n",
    ")\n",
    "\n",
    "df_customer = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .schema(customer_schema) \\\n",
    "    .load('input/customer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/strading\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"customer\",\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").mode(\"overwrite\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# 대표 IT 기업의 티커 목록\n",
    "companies = {\n",
    "    \"Apple\": \"AAPL\",\n",
    "    \"Microsoft\": \"MSFT\",\n",
    "    \"Google\": \"GOOGL\",\n",
    "    \"Amazon\": \"AMZN\",\n",
    "    \"Meta\": \"META\"\n",
    "}\n",
    "\n",
    "# 각 기업의 주식 정보를 저장할 데이터프레임 리스트\n",
    "stock_data = []\n",
    "\n",
    "# 각 기업의 주식 정보를 가져오기\n",
    "for company, ticker in companies.items():\n",
    "    print(f\"Fetching data for {company} ({ticker})...\")\n",
    "    stock = yf.Ticker(ticker)\n",
    "    \n",
    "    # 최근 1개월간의 주식 정보 가져오기\n",
    "    data = stock.history(period=\"1mo\")\n",
    "    data[\"Company\"] = company  # 회사명 추가\n",
    "    stock_data.append(data)\n",
    "\n",
    "\n",
    "print(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pydeequ\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"finance_data_etl\")\\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"com.amazon.deequ:deequ:2.0.7-spark-3.5,\"\n",
    "            \"com.google.cloud.spark:spark-3.5-bigquery:0.41.1,\"\n",
    "            \"mysql:mysql-connector-java:8.0.33\") \\\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord) \\\n",
    "    .config('spark.executorEnv.GOOGLE_APPLICATION_CREDENTIALS', 'myjars/spark2big-323e6547b0e3.json') \\\n",
    "    .config('spark.driverEnv.GOOGLE_APPLICATION_CREDENTIALS', 'myjars/spark2big-323e6547b0e3.json') \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load('input/Finance_data.csv')\n",
    "\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "print(df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"jdbc\").options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/mydb\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"mytable\",\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format('jdbc').options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/mydb\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=\"mytable\",\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").load()\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정합성, 퀄리티"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.analyzers import *\n",
    "\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "                    .onData(df) \\\n",
    "                    .addAnalyzer(Size()) \\\n",
    "                    .addAnalyzer(Completeness(\"age\")) \\\n",
    "                    .run()\n",
    "\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.profiles import *\n",
    "\n",
    "result = ColumnProfilerRunner(spark) \\\n",
    "    .onData(df) \\\n",
    "    .run()\n",
    "\n",
    "for col, profile in result.profiles.items():\n",
    "    print(profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.suggestions import *\n",
    "\n",
    "suggestionResult = ConstraintSuggestionRunner(spark) \\\n",
    "             .onData(df) \\\n",
    "             .addConstraintRule(DEFAULT()) \\\n",
    "             .run()\n",
    "\n",
    "# Constraint Suggestions in JSON format\n",
    "print(suggestionResult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 빅쿼리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"parentProject\", \"spark2big\") # 프로젝트명 명시\n",
    "spark.conf.set(\"credentialsFile\", \"myjars/spark2big-323e6547b0e3.json\")\n",
    "\n",
    "df = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .load(\"bigquery-public-data.samples.shakespeare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 'abc', 24),\n",
    "    (2, 'def', 14),\n",
    "    (3, 'ghu', 4),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"parentProject\", \"spark2big\")\n",
    "spark.conf.set(\"credentialsFile\", \"myjars/spark2big-323e6547b0e3.json\")\n",
    "\n",
    "df.write.format(\"bigquery\")\\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .save(\"mydataset.mytablename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"bigquery\")\\\n",
    "    .option(\"table\", \">\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType\n",
    "from pydeequ.profiles import *\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
    "\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"scratch\")\\\n",
    "    .config('spark.jars.packages', 'com.amazon.deequ:deequ:2.0.7-spark-3.5')\\\n",
    "    .config('spark.jars', 'myjars/packages_jars/mysql-connector-j-8.0.33.jar,myjars/packages_jars/spark-bigquery-with-dependencies_2.12-0.41.1.jar')\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "table_name = 'budget'\n",
    "\n",
    "df = spark.read.format('jdbc').options(\n",
    "    url=\"jdbc:mysql://db-mysql:3306/strading\",\n",
    "    driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "    dbtable=table_name,\n",
    "    user=\"root\",\n",
    "    password=\"1234\"\n",
    ").load()\n",
    "\n",
    "\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 품질 체크\n",
    "check = Check(spark, CheckLevel.Warning, \"Comprehensive Data Quality Check\")\n",
    "check = (\n",
    "    check.isComplete(\"budget_id\", \"No nulls in budget_id\")\n",
    "    .isComplete(\"budget\", \"No nulls in budget\")\n",
    "    .isUnique(\"budget_id\", \"Unique budget_id\")\n",
    "    .isNonNegative(\"budget\", \"Non-negative budget\")\n",
    "    .hasDataType(\"budget_id\", ConstrainableDataTypes.Integral, \"budget_id is an integer\")\n",
    "    .hasDataType(\"budget\", ConstrainableDataTypes.Integral, \"budget is an integer\")\n",
    "    .satisfies(\"budget < 100000000\", \"Valid budget range\")\n",
    ")\n",
    "\n",
    "# 품질 검증 실행\n",
    "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "# checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkResult_df.select('constraint', 'constraint_message').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession 종료\n",
    "spark.sparkContext._gateway.shutdown_callback_server()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pydeequ.checks import Check\n",
    "from pydeequ.verification import VerificationSuite\n",
    "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"deequ_test\") \\\n",
    "    .config('spark.jars.packages', 'com.amazon.deequ:deequ:2.0.7-spark-3.5') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [\n",
    "    {\"budget_id\": 1, \"budget\": 100},\n",
    "    {\"budget_id\": 2, \"budget\": None},\n",
    "    {\"budget_id\": 3, \"budget\": 200},\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "check = Check(spark, CheckLevel.Warning, \"Comprehensive Data Quality Check\")\n",
    "check = (\n",
    "    check.isComplete(\"budget_id\", \"No nulls in budget_id\")\n",
    "    .isComplete(\"budget\", \"No nulls in budget\")\n",
    "    .isUnique(\"budget_id\", \"Unique budget_id\")\n",
    "    .satisfies(\"budget < 100000000\", \"Valid budget range\")\n",
    "    .isNonNegative(\"budget\", \"Non-negative budget\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 품질 검증 실행\n",
    "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "checkResult_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydeequ.f2j_maven_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pydeequ\n",
    "\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.sparkContext.parallelize([\n",
    "            Row(a=\"foo\", b=1, c=5),\n",
    "            Row(a=\"bar\", b=2, c=6),\n",
    "            Row(a=\"baz\", b=3, c=None)]).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(\n",
    "        check.hasSize(lambda x: x >= 3) \\\n",
    "        .hasMin(\"b\", lambda x: x == 0) \\\n",
    "        .isComplete(\"c\")  \\\n",
    "        .isUnique(\"a\")  \\\n",
    "        .isContainedIn(\"a\", [\"foo\", \"bar\", \"baz\"]) \\\n",
    "        .isNonNegative(\"b\") \\\n",
    "        .hasDataType(\"c\", ConstrainableDataTypes.Integral, hint=\"c is an integer\")) \\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.select('constraint_message').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug db to dw final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('connection check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache\n",
      "The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cac802af-de37-457d-b50c-4c3cacd7d857;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.7-spark-3.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;2.1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;2.1.0 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.1 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.1.1 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0 in central\n",
      "\tfound dev.ludovic.netlib#blas;3.0.1 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound dev.ludovic.netlib#lapack;3.0.1 in central\n",
      "\tfound dev.ludovic.netlib#arpack;3.0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.7.0 in central\n",
      ":: resolution report :: resolve 675ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.7-spark-3.5 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tdev.ludovic.netlib#arpack;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#blas;3.0.1 from central in [default]\n",
      "\tdev.ludovic.netlib#lapack;3.0.1 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.7.0 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;2.1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;2.1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.1 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.15 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.2.0 by [org.scala-lang.modules#scala-collection-compat_2.12;2.7.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   23  |   0   |   0   |   3   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cac802af-de37-457d-b50c-4c3cacd7d857\n",
      "\tconfs: [default]\n",
      "\t20 artifacts copied, 0 already retrieved (35014kB/107ms)\n",
      "25/01/01 07:18:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, TimestampType\n",
    "from pydeequ.profiles import *\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pydeequ.verification import VerificationSuite, VerificationResult\n",
    "from pydeequ.checks import Check, CheckLevel, ConstrainableDataTypes\n",
    "\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .appName(\"scratch\")\\\n",
    "    .config('spark.jars.packages', 'com.amazon.deequ:deequ:2.0.7-spark-3.5')\\\n",
    "    .config('spark.jars', 'myjars/packages_jars/mysql-connector-j-8.0.33.jar,myjars/packages_jars/spark-bigquery-with-dependencies_2.12-0.41.1.jar')\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# spark session\n",
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import pydeequ\n",
    "\n",
    "spark.conf.set(\"parentProject\", \"spark2big\")\n",
    "spark.conf.set(\"credentialsFile\", \"myjars/spark2big-992917168560.json\")\n",
    "\n",
    "\n",
    "df_list = ['stocks']\n",
    "\n",
    "for df_name in df_list:\n",
    "    # load data from mysql\n",
    "    df = spark.read.format('jdbc').options(\n",
    "        url=\"jdbc:mysql://db-mysql:3306/strading\",\n",
    "        driver=\"com.mysql.cj.jdbc.Driver\",\n",
    "        dbtable=df_name,\n",
    "        user=\"root\",\n",
    "        password=\"1234\"\n",
    "    ).load()\n",
    "\n",
    "    # save data to bigquery\n",
    "    df.write.format(\"bigquery\")\\\n",
    "        .option(\"writeMethod\", \"direct\") \\\n",
    "        .mode('overwrite') \\\n",
    "        .save(\"mydataset.\" + df_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
